# Direct | Autonomous Data Command Center ğŸŸ¢

A full-stack **Data Engineering Platform** featuring a high-performance Python ETL engine and a cinematic real-time dashboard.

[![Live Demo](https://img.shields.io/badge/Live_Demo-Direct-emerald?style=for-the-badge&logo=vercel)](https://direct-website.onrender.com)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## ğŸŒŸ Overview
**Direct** is a "Command Center" for autonomous data agents. It visualizes the entire lifecycle of dataâ€”from raw extraction to database storageâ€”in a production-ready interface designed for data engineers.

### ğŸ— System Architecture
**Wikipedia** (Source) â” **Python ETL** (Engine) â” **PostgreSQL** (Storage) â” **Next.js Dashboard** (Visualization)

## ğŸ–¥ï¸ Command Center (Frontend)
The dashboard is built for high information density and visual impact.
- **Tech Stack**: Next.js 14, TypeScript, TailwindCSS, Framer Motion.
- **Features**:
  - **Live Audit Stream**: WebSocket-style real-time logs.
  - **Visual Data**: Recharts integration for data quality trends.
  - **Bento Grid Layout**: Optimized for professional workflows.
  - **Glassmorphism UI**: "Deep Charcoal" & Emerald aesthetic.

## âš™ï¸ ETL Engine (Backend)
A production-grade pipeline that extracts financial data, normalizes it, and ensures data integrity.
- **Tech Stack**: Python 3.13, Pandas, SQLAlchemy, PostgreSQL.
- **Extract**: Scrapes HTML tables using `requests` with User-Agent rotation.
- **Transform**: Cleans currency data (`$1,200.50` â†’ `1200.50`), normalizes headers to `snake_case`.
- **Load**: Transactional inserts into PostgreSQL using `SQLAlchemy`.
- **Automation**: Cron-ready shell scripts.

## ğŸ“‚ Project Structure
```bash
data_etl_project/
â”œâ”€â”€ direct/              # Frontend Application (Next.js)
â”‚   â”œâ”€â”€ src/app/         # App Router & Pages
â”‚   â””â”€â”€ src/components/  # Dashboard UI Components
â”œâ”€â”€ etl/                 # Backend ETL Package
â”‚   â”œâ”€â”€ extract.py       # Scrapes data from web
â”‚   â”œâ”€â”€ transform.py     # Cleans and normalizes
â”‚   â”œâ”€â”€ load.py          # Loading logic (Postgres)
â”‚   â””â”€â”€ utils.py         # Shared logging
â”œâ”€â”€ tests/               # Unit Test Suite
â”œâ”€â”€ main.py              # Pipeline Entry Point
â””â”€â”€ run_etl.sh           # Automation Script
```

## ğŸš€ How to Run

### 1. Backend (ETL Pipeline)
```bash
# Setup Python Environment
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# Run the Pipeline
python main.py
```

### 2. Frontend (Dashboard)
```bash
cd direct
npm install
npm run dev
# Open http://localhost:3000
```

## ğŸ“ˆ Key Features
- **Real-time Monitoring**: Watch agents (validators, scrapers) work in real-time.
- **Robust Error Handling**: Gracefully handles network failures and schema changes.
- **Security**: Database credentials managed via `.env`.
- **Data Quality**: Automated unit tests for data transformation logic.

## ğŸ‘¤ Author
[Shambhavi Pandey] - Data Engineering Enthusiast
