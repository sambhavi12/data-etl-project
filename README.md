# Automated Data ETL Pipeline (Wikipedia â†’ PostgreSQL)

A production-grade ETL (Extract, Transform, Load) pipeline that scrapes financial data from Wikipedia, cleans and normalizes it, and loads it into a PostgreSQL database. Designed with **industrial data engineering practices** including modular architecture, automated testing, and secure credential management.

## ğŸ— Architecture
**Wikipedia** (Source) â” **Python Scripts** (Extract/Transform) â” **PostgreSQL** (Destination)

- **Extract**: Scrapes HTML tables using `requests` with custom headers (User-Agent rotation).
- **Transform**: 
  - Standardizes column names (snake_case).
  - Cleans currency data (Regex removal of `$` and `,`).
  - Flattening MultiIndex headers.
- **Load**: Inserts data into PostgreSQL using `SQLAlchemy` (safe paramaterization).
- **Automation**: Shell script wrapper (`run_etl.sh`) for Cron scheduling.

## ğŸ›  Tech Stack
- **Language**: Python 3.13
- **Libraries**: Pandas, SQLAlchemy, Requests, Pytest, Python-Dotenv
- **Database**: PostgreSQL 16+
- **DevOps**: Cron (Automation), Pytest (Unit Testing)

## ğŸ“‚ Project Structure
```bash
data_etl_project/
â”œâ”€â”€ etl/                 # Source Code Package
â”‚   â”œâ”€â”€ extract.py       # Scrapes data from web
â”‚   â”œâ”€â”€ transform.py     # Cleans and normalizes pandas DataFrame
â”‚   â”œâ”€â”€ load.py          # Loading logic (Postgres)
â”‚   â””â”€â”€ utils.py         # Shared logging configuration
â”œâ”€â”€ tests/               # Unit Test Suite
â”‚   â””â”€â”€ test_transform.py
â”œâ”€â”€ main.py              # Entry point for the pipeline
â”œâ”€â”€ run_etl.sh           # Bash wrapper for Cron automation
â”œâ”€â”€ requirements.txt     # Python dependencies
â””â”€â”€ .env                 # Database credentials (Not committed)
```

## ğŸš€ How to Run

### 1. Prerequisites
- Python 3.9+
- PostgreSQL installed and running locally.

### 2. Setup
Clone the repository and install dependencies:
```bash
git clone https://github.com/YourUsername/data-etl-project.git
cd data-etl-project
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### 3. Configuration
Create a `.env` file in the root directory:
```bash
DB_USER=postgres
DB_PASS=your_password
DB_HOST=localhost
DB_PORT=5432
DB_NAME=postgres
```

### 4. Run Pipeline
```bash
python main.py
```
*Check `etl_log.txt` for execution details.*

### 5. Run Tests
```bash
pytest
```

## ğŸ“ˆ Key Features
- **Robust Error Handling**: Handles 403 Forbidden errors and missing columns gracefully.
- **Security**: Database credentials are never hardcoded (using `python-dotenv`).
- **Data Quality**: Unit tests ensure data cleaning logic handles messy formatting (e.g., "$1,200.50").
- **Modular Design**: Separation of concerns allows for easy maintenance and scaling.

## ğŸ‘¤ Author
[Shambhavi Pandey] - Data Engineering Enthusiast
